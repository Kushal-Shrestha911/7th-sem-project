{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [body_text, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = open('nepali_stopwords.txt','r',encoding=\"utf-8\").read().splitlines()\n",
    "negWords = open('neg.csv','r',encoding=\"utf-8\").read().splitlines()\n",
    "posWords = open('positive_words.txt','r',encoding=\"utf-8\").read().splitlines()\n",
    "data = pd.read_csv(\"headlines.txt\",sep=\"#\",header=None)\n",
    "data.columns=[\"body_text\",\"label\"]\n",
    "#negData = data.loc[data['label']=='neg']\n",
    "#data = data.append(negData,ignore_index=True)\n",
    "data = shuffle(data)\n",
    "data.loc[data[\"label\"].isnull()]\n",
    "data.loc[data['label']==\"pos \"]\n",
    "data[\"label\"].unique()\n",
    "#data.shape\n",
    "negData = data.loc[data['label']=='neg ']\n",
    "negData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemword(word):\n",
    "\t\tx = re.findall(r'^((.*?)(लाई|ले|लागि|बाट|देखि|को|की|का|मा|माथि|कै|हरु|हरू|मै|न्ने|सँग|सँगै|वटा))$', word)\n",
    "\t\tif x:\n",
    "\t\t\ty = re.findall(r'^((.*?)(लाई|ले|लागि|बाट|देखि|को|की|का|मा|माथि|कै|हरु|हरू|मै|न्ने|सँग|सँगै|वटा))$', x[0][1])\n",
    "\t\t\tif y:\t\n",
    "\t\t\t\treturn y[0][1]\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn x[0][1]\n",
    "\t\telif word.replace(\"'\",\"\").replace(\" \",'').strip():\n",
    "\t\t\treturn word.replace(\"'\",\"\").replace(\" \",'').strip()\n",
    "\t\telse:\n",
    "\t\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nepali_tokenize(text):\n",
    "    colon_lexicon = ['अंशत:', 'मूलत:', 'सर्वत:', 'प्रथमत:', 'सम्भवत:', 'सामान्यत:', 'विशेषत:', 'प्रत्यक्षत:',\n",
    "        \t\t\t\t'मुख्यत:', 'स्वरुपत:', 'अन्तत:', 'पूर्णत:', 'फलत:', 'क्रमश:', 'अक्षरश:', 'प्रायश:',\n",
    "        \t\t\t\t'कोटिश:', 'शतश:', 'शब्दश:','अत:']\n",
    "\n",
    "        # Handling punctuations: , \" ' ) ( { } [ ] ! ‘ ’ “ ” :- ? । / —\n",
    "    text = re.sub('\\,|\\\"|\\'| \\)|\\(|\\)| \\{| \\}| \\[| \\]|!|‘|’|“|”| \\:-|\\?|।|/|\\—', ' ', text)\n",
    "    words_original = text.split()\n",
    "\n",
    "    words = []\n",
    "    for word in words_original:\n",
    "            if word[len(word) - 1:] == '-':\n",
    "                if not word == '-':\n",
    "                    words.append(word[:len(word) - 1])\n",
    "            else:\n",
    "                if word[len(word) - 1:] == ':' and word not in colon_lexicon:\n",
    "                    words.append(word[:len(word) - 1])\n",
    "                else:\n",
    "                    words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepali_num = re.compile(r'(०|१|२|३|४|५|६|७|८|९)+')\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = nepali_tokenize(text)\n",
    "    text_no_num = [token for token in tokens if  not nepali_num.match(token)]\n",
    "    text = [stemword(word) for word in text_no_num if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negCount(text):\n",
    "    text = clean_text(text)\n",
    "    count = sum([1 for word in text if word in negWords])\n",
    "    return count\n",
    "\n",
    "def posCount(text):\n",
    "    text = clean_text(text)\n",
    "    count = sum([1 for word in text if word in posWords])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>label</th>\n",
       "      <th>negCount</th>\n",
       "      <th>posCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>दूतावासका लागि किनिएका जग्गा अलपत्र</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>आइपीएल : सन्दीपको बलमा पञ्जाबका कुरान आउट</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>गोल्डेन बुटमा लुकाकुको पनि दाबेदारी, रोनाल्डोस...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>रासायनिक मल बिक्री</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>विप्लवलाई प्रतिवन्ध समाधान होइन : कांग्रेस</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              body_text label  negCount  \\\n",
       "3636               दूतावासका लागि किनिएका जग्गा अलपत्र    neg         1   \n",
       "1394          आइपीएल : सन्दीपको बलमा पञ्जाबका कुरान आउट   pos         0   \n",
       "532   गोल्डेन बुटमा लुकाकुको पनि दाबेदारी, रोनाल्डोस...   pos         0   \n",
       "4548                                 रासायनिक मल बिक्री   pos         0   \n",
       "2998         विप्लवलाई प्रतिवन्ध समाधान होइन : कांग्रेस   pos         0   \n",
       "\n",
       "      posCount  \n",
       "3636         0  \n",
       "1394         0  \n",
       "532          0  \n",
       "4548         0  \n",
       "2998         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['negCount'] = data['body_text'].apply(lambda x: negCount(x))\n",
    "data['posCount'] = data['body_text'].apply(lambda x : posCount(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text','negCount','posCount']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text, ngram_range =(1,4))\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['negCount','posCount']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['negCount','posCount']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gb(x, y):\n",
    "\n",
    "    gb = GradientBoostingClassifier(n_estimators=250, max_depth=31, learning_rate = 0.05,\n",
    "                                    max_features='sqrt',subsample = 0.95, random_state =10)\n",
    "    start = time.time()\n",
    "    gb_model = gb.fit(x, y)\n",
    "    end = time.time()\n",
    "    fit_time = end - start\n",
    "    return gb_model, fit_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,fit_time = train_gb(X_train_vect, y_train)\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 122.045 / Predict time: 0.756 ---- Precision: 0.811 / Recall: 0.729 / Accuracy: 0.799\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='neg', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3),\n",
    "        round((y_pred == y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[443, 140],\n",
       "       [ 93, 605]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.76      0.79       583\n",
      "         pos       0.81      0.87      0.84       698\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1281\n",
      "   macro avg       0.82      0.81      0.82      1281\n",
      "weighted avg       0.82      0.82      0.82      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf_vect_fit, open('final_vector.sav', 'wb'))\n",
    "pickle.dump(model, open('final_gbc.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-477fa34615c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
